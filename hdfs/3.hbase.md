## Hbase  
&emsp; Hbase是一个高可靠、高性能、面向列、可伸缩的分布式存储系统。使用hdfs作为文件存储系统，使用zookeeper作为协同服务，使用mapreduce处理海量数据。  
&emsp; Hbase以表的形式存储数据；Table在行的方向上分割为多个HRegion，每个HRegion分散在不同的RegionServer中。
每个HRegion由多个Store构成，每个Store由一个MemStore和0或多个StoreFile组成，每个Store保存一个ColumnsFamily。StoreFile以HFile格式存储在HDFS中。  
### 基本概念    
&emsp; HBase中的存储主要包括HMaster、HRegionServer、HRegion、HLog、Store、MemStore、StoreFile、HFile等。Hbase存储架构图如下：  
![hbase](../images/hdfs/hbase-001.png)  
* HMaster: 为HRegionServer分配HRegion，负责HRegionServer的负载均衡，发现失效的HRegionServer并重新分配，管理HRegion的元数据处理schema更新请求，HDFS上的垃圾文件回收；HMaster在客户端读写操作的时候并不做任何参与；  
* HRegionServer: 维护HMaster分配的HRegion，处理对这些HRegion的IO请求，切分运行过程中变得过大的HRegion；  
* HRegion: Table在行上分割为多个HRegion，HRegion是Hbase中分布式存储和负载均衡的最小单位。HRegion按大小分割，每个表一般只有一个HRegion，随着数据不断插入表，HRegion不断增大，当HRegion的某个列簇达到一个阀值（默认256M）时就会分成两个新的HRegion。  
* Store: 每个HRegion由1个或者多个Store组成，每个ColumnFamily创建一个Store,Store由一个MemStore和多个StoreFile组成，Hbase以Store的大小来判断是否需要切割HRegion，  
* MemStore: 放在内存里，保存修改的key-value。当MemStore达到一个阈值(默认64M)的时候会被Flush到文件，  
* StoreFile: MemStore内存中的数据写道文件后就是StoreFile，StoreFile底层就是HFile格式，  
* HFile: Hbase中key-value存储格式，是hadoop二进制文件格式，
* HLog: write ahead log，记录数据的变更，RegionServer宕机后可以从HLog恢复，每个RegionServer一个HLog文件；  
* RowKey：记录在Hbase中的唯一标识。访问Hbase中的行有三种方式：通过单个rowkey访问、rowkey的range访问、全表扫描。  
### .META表  
&emsp; .META表的rowkey是由Region所在的表名+region的startRowKey+时间戳。而这三者的MD5值也是Hbase存储在HDFS上Region的名字；使用命令describe 'hbase:meta'可以查看meta表结构，scan 'hbase:meta'可以查看meta表的内容。如下所示：  

	describe 'hbase:meta'
	hbase:meta, {
		TABLE_ATTRIBUTES => {
			IS_META => 'true', 
			REGION_REPLICATION => '1', 
			coprocessor$1 => '|org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint|536870911|'
		}                 
	COLUMN FAMILIES DESCRIPTION                                                                                                                                                                  
	{
		NAME => 'info', 
		BLOOMFILTER => 'NONE', 
		VERSIONS => '10', 
		IN_MEMORY => 'true', 
		KEEP_DELETED_CELLS => 'FALSE', 
		DATA_BLOCK_ENCODING => 'NONE', 
		TTL => 'FOREVER', 
		COMPRESSION => 'NONE', 
		CACHE_DATA_IN_L1 => 'true', 
		MIN_VERSIONS => '0', 
		BLOCKCACHE => 'true', 
		BLOCKSIZE => '8192', 
		REPLICATION_SCOPE => '0'
	} 

### 数据存储  
&emsp; 数据存储首先按照rowkey分为不同的HRegion，然后在HRegion内部按照不同的列族分为Store。  

读请求过程：
1. 客户端连接zookeeper，通过节点/hbase/meta-region-server得到meta表所在的Region；   
2. 到meta表所在的Region查找meta表数据，找到目标数据所在的Region；  
3. 定位到目标Region，发送查询请求；
4. Region现在MemStore中查找，如果找到则返回；  
5. 如果MemStore中没有，则取StoreFile中查找，如果由多个StoreFile，可以采用bloomfilter加快查询速度；

1 客户端通过zookeeper以及root表和meta表找到目标数据所在的Regionserver 
2 联系Regionserver查询目标数据 
3 Regionserver定位到目标数据所在的Region，发出查询请求 
4 Region先在MemStore中查找，命中则返回 
5 如果在MemStore中找不到，则在storefile中扫描；如果内部有很多storefile，可以使用bloomfilter

写请求过程： 
1 client向Region server提交写请求 
2 Region server找到目标Region 
3 Region检查数据是否与schema一致 
4 如果客户端没有指定版本，则获取当前系统时间作为数据版本 
5 将更新写入WAL log 
6 将更新写入MemStore 
7 判断MemStore的是否需要flush为Store文件。

### 参数  
* hbase.hregion.memstore.flush.size: memstore达到此值后会flush到文件，默认大小128M；  
* hbase.hstore.flusher.count：MemStore执行flush操作的线程数，默认2个；  
* hbase.hstore.blockingStoreFiles：如果Store中的StoreFile超过了设置的值，所有的更新操作会被阻塞，默认16个；  
* hbase.hstore.blockingWaitTime：Store的更新操作阻塞时间；   
* hbase.hregion.max.filesize：region最大值（默认10G），超过大小会分割为两个Region；  
* hbase.master.logcleaner.ttl: WAL日志的ttl事件，默认10分钟；  
* hbase.hstore.compactionThreshold：Region中的StoreFile超过设置的值后会进行合并操作，默认3个；  
* hbase.hregion.majorcompaction： 默认每7天进行一个major compaction操作；  
### 优化  
* 压缩StoreFile，因为cpu压缩和解压缩效率要比磁盘IO的效率高；  
* 将经常需要一起读取的行放在一起；所有需要一起读取的数据放在同一个列族；    
* 创建Table的时候可以指定基于LRU的BlockCache默认64KB，可以提升HFile中DataBlock的IO效率；  
* 大号的Block有利于顺序scan，小号的block有利于随机查询；  
* Bloom Filters：开启bloomfilter后blockCacheHitRatio会增加，
* minor compaction：将小的storeFile合并成相对较大的storefile，提升查询效率；不会删除过期或者标记删除的数据；  
* major compaction：每个store生成一个StoreFile，删除过期或者标记删除的数据；空闲时手动执行major compaction；    
* 手动执行Region的拆分和StoreFile的合并；
* 热点region拆分为多个region分散到多个RegionServer上；  
* 预拆分region，创建表的时候预先分配多个Region，指定Region存储的rowkey范围；   
* hbase.balancer.period，执行负载均衡，默认每5分钟执行一次；   
* setAutoFlush(false)：客户端有大量写入的时候，数据写入缓存中达到阈值后一次性发送给服务器；  
* 关闭Put的WAL，可能丢失数据；  


https://www.cnblogs.com/steven-note/p/7209398.html