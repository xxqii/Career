## Logstash无法从kafka消费数据
### 问题描述
&emsp;kafka是单节点，启动的时候配置文件指定了offsets.topic.replication.factor=2，创建topic的时候设置replication.factor=1，然后启动logstash从kafka消费数据，但是一直无法消费。  
&emsp;kafka配置文件如下：

	broker.id=1
	delete.topic.enable=true
	num.network.threads=3
	num.io.threads=8
	auto.create.topics.enable=true
	default.replication.factor=1

	socket.send.buffer.bytes=102400
	socket.receive.buffer.bytes=102400
	socket.request.max.bytes=104857600

	log.dirs=/mnt/vdb/magic-mirror/kafka,/mnt/vdc/magic-mirror/kafka
	num.partitions=4

	num.recovery.threads.per.data.dir=1

	offsets.topic.replication.factor=2
	transaction.state.log.replication.factor=1
	transaction.state.log.min.isr=1

	log.flush.interval.messages=10000
	log.flush.interval.ms=10000

	log.retention.hours=120
	log.segment.bytes=134217728
	log.retention.check.interval.ms=300000

	zookeeper.connect=magic-mirror-1:2181,magic-mirror-2:2181,magic-mirror-3:2181
	zookeeper.connection.timeout.ms=6000

&emsp;Logstash配置如下：

	input{
		kafka{
			auto_commit_interval_ms => "3000"
			auto_offset_reset => "earliest"
			bootstrap_servers => ["magic-mirror-1:9092"]
			client_id => "logstash_test"
			group_id => "logstash_test"
			consumer_threads => 2
			decorate_events => true
			enable_auto_commit => "true"
			topics => "slots"
			type => "test"
		}
	}

	output {
		stdout { codec => rubydebug }
	}
### 问题排查
1、启动logstash后日志正常，没有错误日志打印，提示9600端口启动成功；
2、打开logstash的debug日志，然后重启logstash，查看debug日志；

	config.debug: true
	log.level: debug
	
3、查看debug日志，发现logstash启动后打印了大量错误信息，如下所示：

	[2019-03-25T22:09:16,990][DEBUG][org.apache.kafka.clients.consumer.internals.AbstractCoordinator] [Consumer clientId=logstash_test-1, groupId=logstash_test] Received FindCoordinator response ClientResponse(receivedTimeMs=1553566156989, latencyMs=45, disconnected=false, requestHeader=RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=1, clientId=logstash_test-1, correlationId=0), responseBody=FindCoordinatorResponse(throttleTimeMs=0, errorMessage='null', error=COORDINATOR_NOT_AVAILABLE, node=:-1 (id: -1 rack: null)))  
	
*关键信息：COORDINATOR_NOT_AVAILABLE*
4、查看资料，发现COORDINATOR_NOT_AVAILABLE异常可能是由于单机模式下kafka启动时候设置的offsets.topic.replication.factor>1造成的，如下所示：

	https://stackoverflow.com/questions/40316862/the-group-coordinator-is-not-available-kafka
	
5、修改offsets.topic.replication.factor=1，重启kafka，

	./kafka-server-stop.sh 
	./kafka-server-start.sh -daemon ../config/server.properties
	
6、启动logstash，查看日志，一切正常。logstash可以正常消费kafka数据。
