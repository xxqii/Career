### 配置参数  
* batch.size: 默认16384(16k)，客户端一个batch的大小，batch在RecordAccumulator中进行缓存。  
* max.block.ms： 客户端阻塞时间，默认60000(60s)，当客户端buffer满了，或者metadata不可用的时候会阻塞；  
* max.request.size：请求的最大字节数，默认1048576(1m),结合batch.size，可以控制客户端每次发送的batch数量；  
* partitioner.class： 默认DefaultPartitioner，如果没有指定key则采用轮询的方式指定分区；  
* enable.idempotence：默认false，如果设置为true可以实现exactly one语义。max.in.flight.requests.per.connection >= 5；retries > 0 ; acks = 'all'  
* interceptor.classes: 消息发送时候的拦截器链；  
* max.in.flight.requests.per.connection：默认5，同一个客户端发送的没有ack的请求数目；  
* metadata.max.age.ms：metadata在客户端缓存时间，默认5min；  
* transaction.timeout.ms：事务超时时间；  
* transactional.id：事务id；  

### Producer发送消息  
&emsp; interceptors -> serializer key-value -> partition -> accumulator 
1. 遍历拦截器链，调用拦截器里的onSend方法： 
	
	public Future<RecordMetadata> send(ProducerRecord<K, V> record, Callback callback) {
        // intercept the record, which can be potentially modified; this method does not throw exceptions	
        ProducerRecord<K, V> interceptedRecord = this.interceptors.onSend(record);
        return doSend(interceptedRecord, callback);
    }

2. 确定metadata可用  
	
	//KafkaProducer构造的时候会创建meta，并更新内容。
	//默认5min更新一次内容。主线程读，send线程更新
	if (metadata != null) {
		this.metadata = metadata;
	} else {
		this.metadata = new Metadata(retryBackoffMs, config.getLong(ProducerConfig.METADATA_MAX_AGE_CONFIG),
			true, true, clusterResourceListeners);
		this.metadata.update(Cluster.bootstrap(addresses), Collections.emptySet(), time.milliseconds());
	}
			
	//封装了集群中的metadata信息
	ClusterAndWaitTime clusterAndWaitTime;
	try {
		clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), maxBlockTimeMs);
	} catch (KafkaException e) {
		if (metadata.isClosed())
			throw new KafkaException("Producer closed while send in progress", e);
		throw e;
	}
			
3. 序列化key-value：  

	byte[] serializedKey;
	try {
		serializedKey = keySerializer.serialize(record.topic(), record.headers(), record.key());
	} catch (ClassCastException cce) {
		throw new SerializationException("Can't convert key of class " + record.key().getClass().getName() +
				" to class " + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() +
				" specified in key.serializer", cce);
	}
	byte[] serializedValue;
	try {
		serializedValue = valueSerializer.serialize(record.topic(), record.headers(), record.value());
	} catch (ClassCastException cce) {
		throw new SerializationException("Can't convert value of class " + record.value().getClass().getName() +
				" to class " + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() +
				" specified in value.serializer", cce);
	}

4. 确定分区  

	int partition = partition(record, serializedKey, serializedValue, cluster);
	private int partition(ProducerRecord<K, V> record, byte[] serializedKey, byte[] serializedValue, Cluster cluster) {
        Integer partition = record.partition();
        return partition != null ?
                partition :
                partitioner.partition(
                        record.topic(), record.key(), serializedKey, record.value(), serializedValue, cluster);
    }
	
5. 组装callback，包括自定义callback和拦截器里的onAcknowledgement方法  

	Callback interceptCallback = new InterceptorCallback<>(callback, this.interceptors, tp);
	private static class InterceptorCallback<K, V> implements Callback {
        private final Callback userCallback;
        private final ProducerInterceptors<K, V> interceptors;
        private final TopicPartition tp;

        private InterceptorCallback(Callback userCallback, ProducerInterceptors<K, V> interceptors, TopicPartition tp) {
            this.userCallback = userCallback;
            this.interceptors = interceptors;
            this.tp = tp;
        }

        public void onCompletion(RecordMetadata metadata, Exception exception) {
            metadata = metadata != null ? metadata : new RecordMetadata(tp, -1, -1, RecordBatch.NO_TIMESTAMP, Long.valueOf(-1L), -1, -1);
            this.interceptors.onAcknowledgement(metadata, exception);
            if (this.userCallback != null)
                this.userCallback.onCompletion(metadata, exception);
        }
    }

6. RecordAccumulator根据TopicPartition缓存消息  
	
	RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey,
                    serializedValue, headers, interceptCallback, remainingWaitMs);
	if (result.batchIsFull || result.newBatchCreated) {
		log.trace("Waking up the sender since topic {} partition {} is either full or getting a new batch", record.topic(), partition);
		this.sender.wakeup();
	}
			
	//内部实现是CopyOnWriteMap保证线程安全。
	private final ConcurrentMap<TopicPartition, Deque<ProducerBatch>> batches;
	
	//每个TopicPartition分配一个Deque，缓存对应的Batch消息。发送的消息会添加到最后一个batch中，如果batch满了，则创建新的batch
	Deque<ProducerBatch> dq = getOrCreateDeque(tp);  

7. send方法返回Future对象，如果同步发送，则执行future.get阻塞，如果是异步发送，则发送完成后调用callback；  

8. KafkaProducer构造的时候会创建Send线程，从RecordAccumulator获取消息头弄过kafkaClient发送到kafka服务器  

	//在KafkaProducer的构造函数中有如下初始化代码
	this.sender = newSender(logContext, kafkaClient, this.metadata);
	String ioThreadName = NETWORK_THREAD_PREFIX + " | " + clientId;
	this.ioThread = new KafkaThread(ioThreadName, this.sender, true);
	this.ioThread.start();
	
9. 在Send中的run方法中会循环调用sendProducerData方法  
	
	//更新topic的metadata信息
	this.metadata.requestUpdate();
	
	//创建producer request
	Map<Integer, List<ProducerBatch>> batches = this.accumulator.drain(cluster, result.readyNodes, this.maxRequestSize, now);
	//添加到inFlightBatches中
	addToInflightBatches(batches);
	//保证有序性
	if (guaranteeMessageOrder) {
		// Mute all the partitions drained
		for (List<ProducerBatch> batchList : batches.values()) {
			for (ProducerBatch batch : batchList)
				this.accumulator.mutePartition(batch.topicPartition);
		}
	}
	
10. 调用KafkaClient发送request。
	